{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9013a9f3-6592-4ad5-84f4-6cb0acfd255a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Auto-reload setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab32d039-f9c9-4812-b10b-ceb087217ca6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "\n",
    "import sys\n",
    "repo_path = \"/Workspace/Repos/vchhatbar11@outlook.com/ecom-project/src\"\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "from config.storage_config import *\n",
    "from config.catalog_config import *\n",
    "from config.spark_config import configure_spark\n",
    "from utils.logger import log_info, log_metric\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Apply minimal Spark config\n",
    "configure_spark(spark)\n",
    "\n",
    "log_info(f\"Target: {CATALOG_NAME}.{BRONZE_SCHEMA}\")\n",
    "log_info(f\"Source: {BRONZE_ORDER_ITEMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e12e2a-a611-48db-8282-03ececd28d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Check data exists\n",
    "\n",
    "log_info(\"Checking bronze storage...\")\n",
    "\n",
    "files = dbutils.fs.ls(BRONZE_ORDER_ITEMS)\n",
    "\n",
    "# Count CSV files manually\n",
    "csv_count = 0\n",
    "total_bytes = 0\n",
    "\n",
    "for f in files:\n",
    "    if f.name.endswith('.csv') and not f.name.startswith('._'):\n",
    "        csv_count += 1\n",
    "        total_bytes += f.size\n",
    "\n",
    "total_gb = total_bytes / (1024**3)\n",
    "\n",
    "log_info(f\"Found {csv_count} CSV files\")\n",
    "log_info(f\"Total size: {total_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273b94f8-e5d0-499c-9ae0-263f2b0ae04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema explicitly (no inference for performance)\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), nullable=True),\n",
    "    StructField(\"product_id\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "log_info(\"Reading CSV files with explicit schema...\")\n",
    "\n",
    "df = spark.read \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(BRONZE_ORDER_ITEMS)\n",
    "\n",
    "# Cache for reuse\n",
    "df.cache()\n",
    "\n",
    "row_count = df.count()\n",
    "log_metric(\"Rows loaded\", f\"{row_count:,}\")\n",
    "log_info(\"Data cached in memory\")\n",
    "\n",
    "# Quick validation\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "114769d4-b0cd-477c-833b-ff8d7af65b6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data with partition column\n",
    "log_info(\"Adding partition column...\")\n",
    "\n",
    "df_partitioned = df.withColumn(\n",
    "    \"order_id_bucket\",\n",
    "    (col(\"order_id\") / 10000000).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Set catalog context\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Drop existing table (if re-running)\n",
    "table_name = BRONZE_ORDER_ITEMS_TABLE.split('.')[-1]\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "log_info(f\"Dropped existing table: {table_name}\")\n",
    "\n",
    "# Create Iceberg table\n",
    "log_info(\"Creating Iceberg table ...\")\n",
    "\n",
    "df_partitioned.writeTo(table_name) \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .partitionedBy(\"order_id_bucket\") \\\n",
    "    .create()\n",
    "\n",
    "log_info(f\"Iceberg table created: {BRONZE_ORDER_ITEMS_TABLE}\")\n",
    "\n",
    "# Verify row count\n",
    "final_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table_name}\").collect()[0]['cnt']\n",
    "log_metric(\"Table row count\", f\"{final_count:,}\")\n",
    "\n",
    "assert final_count == row_count, \"Row count mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5f89be-2652-4563-b6ae-b7118b2205d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "# Quick validation queries\n",
    "log_info(\"Running validation checks...\")\n",
    "\n",
    "# Check 1: Sample data\n",
    "log_info(\"Sample records:\")\n",
    "spark.sql(f\"SELECT * FROM {BRONZE_ORDER_ITEMS_TABLE} LIMIT 5\").show(truncate=False)\n",
    "\n",
    "# Check 2: Null counts\n",
    "log_info(\"Null value check:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        SUM(CASE WHEN order_id IS NULL THEN 1 ELSE 0 END) as null_order_ids,\n",
    "        SUM(CASE WHEN product_id IS NULL THEN 1 ELSE 0 END) as null_product_ids\n",
    "    FROM {BRONZE_ORDER_ITEMS_TABLE}\n",
    "\"\"\").show()\n",
    "\n",
    "# Check 3: Basic stats\n",
    "log_info(\"Basic statistics:\")\n",
    "spark.sql(f\"SELECT * FROM {BRONZE_ORDER_ITEMS_TABLE}\").describe().show()\n",
    "\n",
    "log_info(\"Validation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7510725-70b5-4c71-ba00-4296529e4481",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 7"
    }
   },
   "outputs": [],
   "source": [
    "# Show table details\n",
    "log_info(\"Table metadata:\")\n",
    "spark.sql(f\"DESCRIBE DETAIL {BRONZE_ORDER_ITEMS_TABLE}\").show(truncate=False)\n",
    "\n",
    "log_info(\"Table schema:\")\n",
    "spark.sql(f\"DESCRIBE {BRONZE_ORDER_ITEMS_TABLE}\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7168337086366745,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_order_items",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
