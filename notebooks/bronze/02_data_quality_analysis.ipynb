{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47c8ac3-f695-4a54-900f-5834f0ea24b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Auto-reload Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c527e8-e4f1-4c82-98cb-d085af0c45a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "repo_path = \"/Workspace/Users/vchhatbar11@outlook.com/ecom-project/src\"\n",
    "sys.path.append(repo_path)\n",
    "\n",
    "from config import *\n",
    "from utils.logger import log_info, log_metric\n",
    "\n",
    "# Cell 2: Analyze partition skew\n",
    "log_info(\"=\" * 60)\n",
    "log_info(\"DATA QUALITY ANALYSIS: Partition Skew\")\n",
    "log_info(\"=\" * 60)\n",
    "\n",
    "# Distribution analysis\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        FLOOR(order_id / 1000000) as order_id_millions,\n",
    "        COUNT(*) as row_count,\n",
    "        ROUND(COUNT(*) * 100.0 / 800000000, 2) as pct_of_total\n",
    "    FROM {BRONZE_ORDER_ITEMS_TABLE}\n",
    "    GROUP BY order_id_millions\n",
    "    ORDER BY order_id_millions\n",
    "\"\"\").show(100, truncate=False)\n",
    "\n",
    "# Cell 3: Skew metrics\n",
    "skew_stats = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        order_id_bucket,\n",
    "        COUNT(*) as rows,\n",
    "        ROUND(COUNT(*) / 1000000.0, 2) as rows_millions\n",
    "    FROM {BRONZE_ORDER_ITEMS_TABLE}\n",
    "    GROUP BY order_id_bucket\n",
    "    ORDER BY rows DESC\n",
    "\"\"\")\n",
    "\n",
    "skew_stats.show(20, truncate=False)\n",
    "\n",
    "max_partition = skew_stats.collect()[0]\n",
    "log_metric(\"Largest partition rows\", f\"{max_partition['rows']:,}\")\n",
    "log_metric(\"Skew factor\", f\"{max_partition['rows'] / 10000000:.1f}x\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_data_quality_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
