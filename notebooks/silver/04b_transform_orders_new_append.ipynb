{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1fb460-bcbc-4de4-a070-10f84400ec51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from config.table_config import PROJECT_PATH\n",
    "\n",
    "if PROJECT_PATH not in sys.path:\n",
    "    sys.path.append(PROJECT_PATH)\n",
    "\n",
    "from config import *\n",
    "from utils.logger import log_info, log_metric\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "from pyspark.sql.types import StringType, DoubleType, TimestampType\n",
    "\n",
    "configure_spark(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "613087b7-52a4-4e2c-b4b2-baaf704b15f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Transform and append orders_new\n",
    "from utils.data_quality_check import run_all_checks\n",
    "\n",
    "log_info(\"=\" * 60)\n",
    "log_info(\"APPEND ORDERS_NEW TO SILVER.ORDERS\")\n",
    "log_info(\"=\" * 60)\n",
    "\n",
    "# Read orders_new\n",
    "log_info(\"Reading orders_new from bronze...\")\n",
    "orders_new_df = spark.table(BRONZE_ORDERS_NEW_TABLE)\n",
    "\n",
    "# DQA on orders_new (1% sample)\n",
    "log_info(\"Running DQA on orders_new (1% sample)...\")\n",
    "sample = orders_new_df.sample(fraction=0.01, seed=42)\n",
    "run_all_checks(\n",
    "    df=sample,\n",
    "    table_name=BRONZE_ORDERS_NEW_TABLE,\n",
    "    key_columns=[\"order_id\"],\n",
    "    partition_col=None\n",
    ")\n",
    "\n",
    "# Transform: Normalize schema\n",
    "log_info(\"Normalizing orders_new schema...\")\n",
    "orders_new_normalized = orders_new_df \\\n",
    "    .withColumn(\"order_id\", col(\"order_id\").cast(StringType())) \\\n",
    "    .withColumn(\"order_date\", col(\"order_date\").cast(TimestampType())) \\\n",
    "    .withColumn(\"order_total_price\", lit(None).cast(DoubleType())) \\\n",
    "    .withColumn(\"source_system\", lit(\"new_system\")) \\\n",
    "    .drop(\"order_year\", \"order_month\") \\\n",
    "    .repartition(32)\n",
    "\n",
    "# Deduplicate\n",
    "log_info(\"Deduplicating orders_new...\")\n",
    "orders_new_clean = orders_new_normalized.dropDuplicates([\"order_id\"])\n",
    "\n",
    "# Add metadata\n",
    "orders_new_clean = orders_new_clean \\\n",
    "    .withColumn(\"ingested_at\", current_timestamp()) \\\n",
    "    .withColumn(\"data_source\", lit(\"ecommerce.bronze.orders_new\"))\n",
    "\n",
    "log_info(\"Orders_new schema:\")\n",
    "orders_new_clean.printSchema()\n",
    "\n",
    "# APPEND to existing silver.orders table\n",
    "log_info(\"Appending to existing silver.orders table...\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "\n",
    "# Append using INSERT INTO (Iceberg supports this)\n",
    "orders_new_clean.writeTo(\"orders\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .append()\n",
    "\n",
    "log_info(\"Orders_new appended to silver.orders\")\n",
    "\n",
    "# Show final counts\n",
    "final_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {SILVER_ORDERS_TABLE}\").collect()[0]['cnt']\n",
    "log_metric(\"Total orders in silver\", f\"{final_count:,}\")\n",
    "log_metric(\"Expected\", \"~1,900,000,000 (700M legacy + 1.2B new)\")\n",
    "\n",
    "log_info(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b3c1b5-14ae-459a-899f-fa9b7d67fbf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get final silver.orders metadata\n",
    "final_detail = spark.sql(\"DESCRIBE DETAIL ecommerce.silver.orders\").collect()[0]\n",
    "\n",
    "final_gb = final_detail['sizeInBytes'] / (1024**3)\n",
    "final_files = final_detail['numFiles']\n",
    "final_rows = 1_532_110_121\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SILVER.ORDERS METRICS (MERGED)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total rows:  {final_rows:,}\")\n",
    "print(f\"Total size:  {final_gb:.2f} GB\")\n",
    "print(f\"Total files: {final_files}\")\n",
    "print()\n",
    "\n",
    "# Calculate breakdown\n",
    "legacy_rows = 700_000_000\n",
    "orders_new_rows = final_rows - legacy_rows\n",
    "\n",
    "print(\"BREAKDOWN:\")\n",
    "print(f\"Legacy orders:     {legacy_rows:,} rows\")\n",
    "print(f\"Orders_new clean:  {orders_new_rows:,} rows\")\n",
    "print(f\"Orders_new removed: ~6,380,000 duplicates (from 1.2B)\")\n",
    "print()\n",
    "\n",
    "# Calculate size contribution\n",
    "legacy_gb = 25.56\n",
    "orders_new_gb = final_gb - legacy_gb\n",
    "\n",
    "print(\"SIZE BREAKDOWN:\")\n",
    "print(f\"Legacy contribution:     {legacy_gb:.2f} GB\")\n",
    "print(f\"Orders_new contribution: {orders_new_gb:.2f} GB\")\n",
    "print(f\"Total:                   {final_gb:.2f} GB\")\n",
    "print()\n",
    "\n",
    "# Verify source distribution\n",
    "print(\"SOURCE SYSTEM DISTRIBUTION:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT source_system, COUNT(*) as count\n",
    "    FROM ecommerce.silver.orders\n",
    "    GROUP BY source_system\n",
    "    ORDER BY source_system\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04b_transform_orders_new_append",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
